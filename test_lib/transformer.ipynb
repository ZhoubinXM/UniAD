{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer 实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. FNN 基于位置的前馈神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFNN(nn.Module):\n",
    "  \"\"\"基于位置的前馈神经网络\"\"\"\n",
    "  def __init__(self, ffn_num_input, ffn_num_hidden, ffn_num_outputs, **kwargs):\n",
    "    super(PositionWiseFNN, self).__init__(**kwargs)\n",
    "\n",
    "    self.dense1 = nn.Linear(ffn_num_input, ffn_num_hidden)\n",
    "    self.relu = nn.ReLU()\n",
    "    self.dense2 = nn.Linear(ffn_num_hidden, ffn_num_outputs)\n",
    "\n",
    "  def forward(self, X):\n",
    "    return self.dense2(self.relu(self.dense1(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FFN Test case\n",
    "ffn = PositionWiseFNN(4, 4, 8)\n",
    "ffn.eval()\n",
    "test_input = torch.ones((2,3,4))\n",
    "ffn(test_input)\n",
    "# ffn.dense1(test_input), ffn.relu(ffn.dense1(test_input)), ffn.dense2(ffn.relu(ffn.dense1(test_input)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Add & Norm block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNorm(nn.Module):\n",
    "    \"\"\"残差连接后进行层归一化\"\"\"\n",
    "    def __init__(self, normalized_shape, dropout, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.ln = nn.LayerNorm(normalized_shape)\n",
    "\n",
    "    def forward(self, X, Y):\n",
    "        \"\"\"X means block input, Y means blocks output\"\"\"\n",
    "        return self.ln(self.dropout(Y) + X)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test case\n",
    "add_norm = AddNorm([3, 4], 0.1)\n",
    "add_norm.eval()\n",
    "# add_norm(torch.ones(2,3,4), torch.ones(2,3,4))\n",
    "input = torch.ones(2,3,4)\n",
    "add_norm.ln(add_norm.dropout(input) + input)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. MultiHeadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_mask(X, valid_lens, value=0):\n",
    "    \"\"\"mask a sequence with mask value\"\"\"\n",
    "    # X: 2D dim(batch, feature), valid_lens \n",
    "    mask_len = X.size(1)\n",
    "    mask = torch.arange((mask_len), dtype=torch.float32, device=X.device)[None, :] < valid_lens[:, None]\n",
    "    X[~mask] = value\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_softmax(X, valid_lens):\n",
    "  \"\"\"通过在最后一个轴上掩蔽元素来执行softmax操作\"\"\"\n",
    "    # X:3D张量，valid_lens:1D或2D张量\n",
    "  if valid_lens is None:\n",
    "      return nn.functional.softmax(X, dim=-1)\n",
    "  else:\n",
    "      shape = X.shape\n",
    "      if valid_lens.dim() == 1:\n",
    "          valid_lens = torch.repeat_interleave(valid_lens, shape[1])\n",
    "      else:\n",
    "          valid_lens = valid_lens.reshape(-1)\n",
    "      # 最后一轴上被掩蔽的元素使用一个非常大的负值替换，从而其softmax输出为0\n",
    "      X = sequence_mask(X.reshape(-1, shape[-1]), valid_lens,\n",
    "                            value=-1e6)\n",
    "      return nn.functional.softmax(X.reshape(shape), dim=-1)\n",
    "\n",
    "# masked_softmax(torch.rand(2, 2, 4), torch.tensor([2, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uniad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17 (default, Jul  5 2023, 21:04:15) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "549d4bc81ae57afb7965088e353b61b9dd1bc5086e16f0efaf01a7389c04c744"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
